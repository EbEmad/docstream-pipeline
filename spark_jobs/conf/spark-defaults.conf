# Application Settings
spark.app.name=DocStream Pipeline

# Dynamic Executor Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=2
spark.dynamicAllocation.initialExecutors=2

# Executor Settings (per executor)
spark.executor.memory=1g
spark.executor.cores=1

# Driver Settings
spark.driver.memory=512m

spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,org.apache.iceberg:iceberg-aws-bundle:1.4.0,org.postgresql:postgresql:42.6.0,io.openlineage:openlineage-spark_2.12:1.22.0,org.opensearch.client:opensearch-spark-30_2.12:1.2.0,org.apache.spark:spark-avro_2.12:3.5.5

# Shuffle Settings
spark.sql.shuffle.partitions=4

# Adaptive Execution
spark.sql.adaptive.enabled=false

# Iceberg Extensions
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Iceberg REST Catalog
spark.sql.catalog.docstream_catalog=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.docstream_catalog.type=rest
spark.sql.catalog.docstream_catalog.uri=http://rest:8181
spark.sql.catalog.docstream_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.docstream_catalog.warehouse=s3://warehouse/
spark.sql.catalog.docstream_catalog.s3.endpoint=http://minio:9000
spark.sql.catalog.docstream_catalog.s3.path-style-access=true
spark.sql.catalog.docstream_catalog.s3.access-key-id=admin
spark.sql.catalog.docstream_catalog.s3.secret-access-key=password

# Hadoop S3A Configuration
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=password
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false

# Kafka Streaming
spark.sql.streaming.kafka.useDeprecatedOffsetFetching=false
spark.sql.streaming.triggerInterval=10 seconds

# S3A Performance Optimizations
spark.hadoop.fs.s3a.committer.name=magic
spark.hadoop.fs.s3a.committer.magic.enabled=true
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.buffer=disk
spark.hadoop.fs.s3a.multipart.size=104857600
spark.hadoop.fs.s3a.threads.max=20
spark.hadoop.fs.s3a.change.detection.mode=none
spark.hadoop.fs.s3a.change.detection.version.required=false

# Kafka Streaming
spark.streaming.stopGracefullyOnShutdown=true
spark.sql.streaming.kafka.useDeprecatedOffsetFetching=false

# Streaming Checkpoints - Optimized
spark.sql.streaming.checkpointLocation.minBatchesToRetain=5
spark.sql.streaming.minBatchesToRetain=5

# MINIO CHECKPOINT MANAGER
#spark.sql.streaming.checkpointFileManagerClass=io.minio.spark.checkpoint.S3BasedCheckpointFileManager

# OpenSearch Configuration (global defaults)
spark.opensearch.nodes=opensearch
spark.opensearch.port=9200
spark.opensearch.net.ssl=false
spark.opensearch.nodes.wan.only=true
spark.opensearch.nodes.discovery=false
spark.opensearch.batch.size.entries=1000
spark.opensearch.batch.size.bytes=10mb
spark.opensearch.batch.write.refresh=false
spark.opensearch.index.auto.create=true

# OpenLineage Integration (disabled - marquez service not running)
# spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener
# spark.openlineage.transport.type=http
# spark.openlineage.transport.url=http://marquez:5000
# spark.openlineage.namespace=docstream
# spark.openlineage.dataset.removePath=true